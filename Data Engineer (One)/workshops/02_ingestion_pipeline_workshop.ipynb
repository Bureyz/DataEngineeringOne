{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d4c82c-7f1d-49f8-b21e-c51803e42ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop 2: Ingestion Pipeline - COPY INTO & Auto Loader\n",
    "\n",
    "**Workshop Objectives:**\n",
    "- Implement batch ingestion using COPY INTO\n",
    "- Configure Auto Loader for streaming ingestion\n",
    "- Handle various file formats (CSV, JSON, Parquet)\n",
    "- Monitor and manage ingestion pipelines\n",
    "\n",
    "**Duration:** 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Theoretical Introduction\n",
    "\n",
    "**Section Objective:** Understanding data ingestion methods in Databricks Lakehouse\n",
    "\n",
    "### COPY INTO - Batch Ingestion\n",
    "- **Purpose**: Load data from external files into Delta tables\n",
    "- **Idempotency**: Automatically tracks processed files, preventing duplicates\n",
    "- **Use case**: Scheduled batch jobs, one-time data migrations\n",
    "- **Supported formats**: CSV, JSON, Parquet, Avro, ORC, TEXT\n",
    "\n",
    "### Auto Loader - Streaming Ingestion\n",
    "- **Purpose**: Incrementally process new files as they arrive\n",
    "- **cloudFiles format**: Uses `.format(\"cloudFiles\")` for streaming read\n",
    "- **Schema inference**: Automatically detects and evolves schema\n",
    "- **Use case**: Near real-time processing, continuous data pipelines\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader |\n",
    "|---------|-----------|-------------|\n",
    "| Processing | Batch | Streaming |\n",
    "| File tracking | Built-in | Checkpoint-based |\n",
    "| Schema evolution | Manual | Automatic |\n",
    "| Scalability | Medium | High |\n",
    "| Cost | Per execution | Per file |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "688d9145-9e56-4a6a-8c38-4afa05d4f506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“š Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b37069fc-397f-4e8a-add1-4b57627b756f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6f7ac52-61fb-426a-8181-df7e3907d901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "SCHEMA = \"bronze\"\n",
    "CHECKPOINT_PATH = \"/Volumes/ecommerce_platform_trainer/default/dataset/workshop\"\n",
    "\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SCHEMA}\")\n",
    "\n",
    "# ÅšcieÅ¼ki do plikÃ³w danych (juÅ¼ zdefiniowane w 00_setup)\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f442eb73-d380-4ed6-a02e-0bbb8382e038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Task 1.1: CSV File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare target table `bronze_customers_batch`\n",
    "2. Use `COPY INTO` to load data from `customers.csv`\n",
    "3. Verify the number of loaded records\n",
    "\n",
    "**Hints:**\n",
    "- Use `BRONZE_PATH` variable for table location\n",
    "- Use `SOURCE_DATA_PATH` for source file path\n",
    "- Format options: `header`, `inferSchema` should be `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f00f5c-d2be-4fb8-acf6-078b147547f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        phone STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        registration_date DATE,\n",
    "        customer_segment STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80a7a0a8-ba79-4775-ab4b-03d353595f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from CSV file\n",
    "spark.sql(f\"\"\"\n",
    "    ____ INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone,\n",
    "            city,\n",
    "            state,\n",
    "            country,\n",
    "            TO_DATE(registration_date) as registration_date,\n",
    "            customer_segment,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}'  -- Complete with CUSTOMERS_CSV variable\n",
    "    )\n",
    "    FILEFORMAT = ____  -- Complete format (CSV)\n",
    "    FORMAT_OPTIONS (\n",
    "        'header' = '____',      -- Does file have header?\n",
    "        'inferSchema' = '____'  -- Infer schema?\n",
    "    )\n",
    "    COPY_OPTIONS (\n",
    "        'mergeSchema' = '____'  -- Merge schema?\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d90c963e-c340-4dde-ba98-a31e33dc6213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3865e32c-eed9-409c-9a24-d848d91c3a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.2: JSON File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare table `bronze_orders_batch`\n",
    "2. Use `COPY INTO` to load data from `orders_batch.json`\n",
    "3. Handle nested JSON structure\n",
    "\n",
    "**Hints:**\n",
    "- Use `DELTA` as table format\n",
    "- Use `BRONZE_PATH` for location\n",
    "- FILEFORMAT should be `JSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aca2b98-9c54-44c3-a573-796c6ebb404a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create table for orders\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent DOUBLE,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING ____  -- Complete format (DELTA)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d04570e8-5e20-47c0-808b-da049b97e372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from JSON file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            product_id,\n",
    "            store_id,\n",
    "            TO_TIMESTAMP(order_datetime) as order_datetime,\n",
    "            quantity,\n",
    "            unit_price,\n",
    "            discount_percent,\n",
    "            total_amount,\n",
    "            payment_method,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}'  -- Complete with ORDERS_JSON variable\n",
    "    )\n",
    "    FILEFORMAT = ____  -- Complete format (JSON)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14be647d-8e5f-4bba-b922-e692080b1a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_batch LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bc7bd0-faea-46a5-9b36-ec8ce9c0a56c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.3: Parquet File Ingestion\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare table `bronze_products_batch`\n",
    "2. Use `COPY INTO` to load data from `products.parquet`\n",
    "3. Add column with source file metadata\n",
    "\n",
    "**Hints:**\n",
    "- Use `_metadata.file_path` to get source file path\n",
    "- FILEFORMAT should be `PARQUET`\n",
    "- Source file: `products.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c639388-27c0-4fd6-9755-6276440ee6f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create table for products\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    "        product_id STRING,\n",
    "        product_name STRING,\n",
    "        subcategory_code STRING,\n",
    "        brand STRING,\n",
    "        unit_cost DOUBLE,\n",
    "        list_price DOUBLE,\n",
    "        weight_kg DOUBLE,\n",
    "        status STRING,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f5595ec-b2df-498a-920b-0aad6ec93cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: COPY INTO from Parquet file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            subcategory_code,\n",
    "            brand,\n",
    "            unit_cost,\n",
    "            list_price,\n",
    "            weight_kg,\n",
    "            status,\n",
    "            ____ as _source_file,  -- Use _metadata.file_path\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}'  -- Complete with PRODUCTS_PARQUET variable\n",
    "    )\n",
    "    FILEFORMAT = ____  -- Complete format (PARQUET)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e91d3acd-6d11-416d-aac1-14892372cc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, brand, list_price, _source_file \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_products_batch \n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd5bc4b-9135-4dc2-ab54-72632c1cb5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 1.4: Idempotency - Re-running COPY INTO\n",
    "\n",
    "**Instructions:**\n",
    "1. Re-run `COPY INTO` for the same table\n",
    "2. Verify that data was not duplicated\n",
    "3. Check `COPY INTO` operation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a60ed36-4099-4868-9c88-4e8af78f6cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check record count before re-running COPY INTO\n",
    "before_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Record count before re-running COPY INTO: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a7369e-6c60-4acb-9c84-b7bb85c0f557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Re-execute COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone,\n",
    "            city,\n",
    "            state,\n",
    "            country,\n",
    "            TO_DATE(registration_date) as registration_date,\n",
    "            customer_segment,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{CUSTOMERS_CSV}'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7654f9b-3923-43cf-b534-e1c6f3495e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Idempotency verification\n",
    "after_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Record count after re-running COPY INTO: {after_count}\")\n",
    "print(f\"Was data duplicated? {before_count != after_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f3e153-9ce9-4379-bf76-7c6e3a8d2382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Task 2.1: Configuring Auto Loader for CSV\n",
    "\n",
    "**Instructions:**\n",
    "1. Prepare checkpoint location\n",
    "2. Use `.format(\"cloudFiles\")` to create streaming read\n",
    "3. Configure schema inference and evolution\n",
    "4. Write stream to `bronze_customers_stream` table\n",
    "\n",
    "**Hints:**\n",
    "- Format: `cloudFiles`\n",
    "- cloudFiles.format: `csv`\n",
    "- Schema location: use `CHECKPOINT_PATH` + subfolder name\n",
    "- Use `current_timestamp()` and `input_file_name()` for metadata\n",
    "- Output mode: `append`, mergeSchema: `true`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0df36b-d8bb-4c56-a3f7-a50e992a4425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Streaming read with Auto Loader\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")  # Complete format (cloudFiles)\n",
    "    .option(\"cloudFiles.format\", \"____\")  # Source file format (csv)\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/____\")  # Schema checkpoint\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c8e493-0ccd-4f0b-9f26-263828b42443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Add metadata columns\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_enriched = (\n",
    " customers_stream\n",
    " .withColumn(\"_ingestion_timestamp\", ____) # Add timestamp\n",
    " .withColumn(\"_source_file\", ____) # Add source file name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0347f823-6dbc-4e68-9840-70ce54218306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write stream to Delta table\n",
    "query_customers = (\n",
    " customers_enriched.writeStream\n",
    " .format(\"____\") # Complete format\n",
    " .outputMode(\"____\") # Write mode (append)\n",
    " .option(\"checkpointLocation\", f\"{____}/customers_stream\") # Checkpoint\n",
    " .option(\"mergeSchema\", \"____\") # Schema evolution\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e8c53d0-a68e-435b-8d23-c3d6173f593c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stream verification\n",
    "import time\n",
    "time.sleep(10) # Wait for processing\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c86c6f-e482-4295-9783-37c628706303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.2: Auto Loader for JSON with Schema Hints\n",
    "\n",
    "**Instructions:**\n",
    "1. Use Auto Loader to read `orders_batch.json`\n",
    "2. Add schema hints for columns with specific types\n",
    "3. Configure rescue data column for invalid records\n",
    "\n",
    "**Hints:**\n",
    "- cloudFiles.format: `json`\n",
    "- schemaHints example: `\"order_date DATE, total_amount DOUBLE\"`\n",
    "- rescuedDataColumn: `_rescued_data`\n",
    "- Checkpoint subfolder: `orders_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98124e6-95c9-4f35-84e4-2aae62dbad9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Auto Loader with schema hints\n",
    "orders_stream = (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"____\") # JSON format\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    " .option(\"cloudFiles.schemaHints\", \"____\") # Hint: \"order_date DATE, total_amount DOUBLE\"\n",
    " .option(\"cloudFiles.rescuedDataColumn\", \"____\") # Column for rescue data\n",
    " .load(f\"{____}/orders_batch.json\") # Complete path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae5bcafe-756d-4884-a275-131479aa6b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Write stream\n",
    "query_orders = (\n",
    " orders_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .outputMode(\"____\") # Append mode\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\") # Checkpoint\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acd89018-c789-4e27-88f4-b05f30b40702",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "time.sleep(10)\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_stream LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8319b3e0-4d58-47a7-8210-89dfabe04c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.3: Monitoring Streaming Queries\n",
    "\n",
    "**Instructions:**\n",
    "1. Display active streaming queries\n",
    "2. Check status and last progress of each query\n",
    "3. Retrieve metrics: number of processed records, batch duration\n",
    "\n",
    "**Hints:**\n",
    "- Use `spark.streams.active` to get active streams\n",
    "- Use `stream.lastProgress` to get last progress metrics\n",
    "- Progress contains: `batchId`, `numInputRows`, `batchDuration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95729d51-fb30-40b6-b40b-ba57615c4675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Display active streams\n",
    "active_streams = spark.streams.____ # Complete method (active)\n",
    "\n",
    "print(f\"Number of active streams: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    " print(f\"\\nStream ID: {stream.id}\")\n",
    " print(f\"Name: {stream.name}\")\n",
    " print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c201beb-34a2-44c8-a6f0-2db864b368bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Check last progress\n",
    "if len(active_streams) > 0:\n",
    " last_progress = active_streams[0].____ # Complete method (lastProgress)\n",
    " \n",
    " if last_progress:\n",
    " print(f\"Batch ID: {last_progress['batchId']}\")\n",
    " print(f\"Processed records: {last_progress['numInputRows']}\")\n",
    " print(f\"Processing time: {last_progress['batchDuration']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb362e63-ff77-4024-b975-2aadaf6a070a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Task 2.4: Stopping Streaming Queries\n",
    "\n",
    "**Instructions:**\n",
    "1. Stop all active streaming queries\n",
    "2. Verify that all streams are stopped\n",
    "\n",
    "**Hints:**\n",
    "- Use `stream.stop()` method to stop a stream\n",
    "- Iterate over `spark.streams.active` to stop all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f728202-09d2-4683-8543-2437becf5208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Stop all streams\n",
    "for stream in spark.streams.active:\n",
    " print(f\"Stopping stream: {stream.name}\")\n",
    " stream.____() # Complete method (stop)\n",
    "\n",
    "print(\"\\nAll streams stopped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3770f3d7-22dc-450d-9168-699bdb84d602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(f\"Number of active streams: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993537b1-cee1-46e5-bb23-98e29d9f34aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: COPY INTO vs Auto Loader Comparison\n",
    "\n",
    "### Task 3.1: Performance Analysis\n",
    "\n",
    "**Instructions:**\n",
    "1. Compare record counts loaded by COPY INTO vs Auto Loader\n",
    "2. Check operation history for both methods\n",
    "3. Identify use cases for each method\n",
    "\n",
    "**Hints:**\n",
    "- Use `DESCRIBE HISTORY` to check table history\n",
    "- Compare `version`, `operation`, `operationMetrics` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8954aced-8f65-48eb-ae81-b633b6161921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Record count comparison\n",
    "copy_into_count = spark.sql(f\"\"\"\n",
    " SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\")\n",
    "\n",
    "auto_loader_count = spark.sql(f\"\"\"\n",
    " SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "copy_into_count.union(auto_loader_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d04b25-29d8-4417-a7d3-73d0a378aaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: COPY INTO operation history\n",
    "spark.sql(f\"\"\"\n",
    " ____ HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e14277a-0141-431d-9b61-3e5906e20d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Auto Loader operation history\n",
    "spark.sql(f\"\"\"\n",
    " DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2cc9697-f56a-4062-bd00-d1bf364923d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Workshop Summary\n",
    "\n",
    "**Achieved Objectives:**\n",
    "- Batch ingestion implementation with COPY INTO\n",
    "- Auto Loader configuration for streaming ingestion\n",
    "- Handling different formats (CSV, JSON, Parquet)\n",
    "- Pipeline monitoring and management\n",
    "\n",
    "**When to Use COPY INTO:**\n",
    "- Batch processing with defined schedule\n",
    "- Known and stable data structure\n",
    "- Need for control over loading process\n",
    "- Out-of-the-box idempotency\n",
    "\n",
    "**When to Use Auto Loader:**\n",
    "- Near real-time processing\n",
    "- Schema evolution and automatic inference\n",
    "- Continuous monitoring for new files\n",
    "- Scalability and cost efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dbec045-d842-4640-8596-7d854a7186be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Solutions\n",
    "\n",
    "Below are the complete solutions for all workshop tasks. Use them to verify your work or if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba14ca9-bee6-47a6-b621-40d70c02d8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SOLUTIONS - Part 1: COPY INTO - Batch Ingestion\n",
    "# =============================================================================\n",
    "\n",
    "# Task 1.1: CSV File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create target table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    "        customer_id STRING,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        phone STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        registration_date DATE,\n",
    "        customer_segment STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from CSV file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            email,\n",
    "            phone,\n",
    "            city,\n",
    "            state,\n",
    "            country,\n",
    "            TO_DATE(registration_date) as registration_date,\n",
    "            customer_segment,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{CUSTOMERS_CSV}'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS (\n",
    "        'header' = 'true',\n",
    "        'inferSchema' = 'true'\n",
    "    )\n",
    "    COPY_OPTIONS (\n",
    "        'mergeSchema' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Task 1.2: JSON File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create table for orders\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent DOUBLE,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from JSON file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            product_id,\n",
    "            store_id,\n",
    "            TO_TIMESTAMP(order_datetime) as order_datetime,\n",
    "            quantity,\n",
    "            unit_price,\n",
    "            discount_percent,\n",
    "            total_amount,\n",
    "            payment_method,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{ORDERS_JSON}'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "\"\"\")\n",
    "\n",
    "# Task 1.3: Parquet File Ingestion\n",
    "# -----------------------------------------------------------------------------\n",
    "# Create table for products\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    "        product_id STRING,\n",
    "        product_name STRING,\n",
    "        subcategory_code STRING,\n",
    "        brand STRING,\n",
    "        unit_cost DOUBLE,\n",
    "        list_price DOUBLE,\n",
    "        weight_kg DOUBLE,\n",
    "        status STRING,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO from Parquet file\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            subcategory_code,\n",
    "            brand,\n",
    "            unit_cost,\n",
    "            list_price,\n",
    "            weight_kg,\n",
    "            status,\n",
    "            _metadata.file_path as _source_file,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{PRODUCTS_PARQUET}'\n",
    "    )\n",
    "    FILEFORMAT = PARQUET\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Part 2: Auto Loader - Streaming Ingestion\n",
    "# =============================================================================\n",
    "\n",
    "# Task 2.1: Configuring Auto Loader for CSV\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# Streaming read with Auto Loader\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/customers_schema\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Add metadata columns\n",
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"_source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write stream to Delta table\n",
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/customers_stream\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")\n",
    "\n",
    "# Task 2.2: Auto Loader for JSON with Schema Hints\n",
    "# -----------------------------------------------------------------------------\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"order_datetime TIMESTAMP, total_amount DOUBLE\")\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"_rescued_data\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# Write stream\n",
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/orders_stream\")\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")\n",
    "\n",
    "# Task 2.3: Monitoring Streaming Queries\n",
    "# -----------------------------------------------------------------------------\n",
    "# Display active streams\n",
    "active_streams = spark.streams.active\n",
    "\n",
    "print(f\"Number of active streams: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status}\")\n",
    "\n",
    "# Check last progress\n",
    "if len(active_streams) > 0:\n",
    "    last_progress = active_streams[0].lastProgress\n",
    "    \n",
    "    if last_progress:\n",
    "        print(f\"Batch ID: {last_progress['batchId']}\")\n",
    "        print(f\"Processed records: {last_progress['numInputRows']}\")\n",
    "        print(f\"Processing time: {last_progress['batchDuration']} ms\")\n",
    "\n",
    "# Task 2.4: Stopping Streaming Queries\n",
    "# -----------------------------------------------------------------------------\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\nAll streams stopped!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOLUTIONS - Part 3: COPY INTO vs Auto Loader Comparison\n",
    "# =============================================================================\n",
    "\n",
    "# Task 3.1: COPY INTO operation history\n",
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0302b16-e596-40c6-a57f-458f9626e15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Resource Cleanup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9550bb-87b7-4467-9de1-138e13e31545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: Run only if you want to delete all created tables\n",
    "\n",
    "# Uncomment the lines below to delete tables:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    "\n",
    "# Clean up checkpoints\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "print(\"Resource cleanup is commented out. Uncomment to delete tables.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ingestion_pipeline_workshop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
